import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
from typing import Dict, List, Tuple

class BERTEmotionDetector:
    """BERT κΈ°λ° κ°μ • λ¶„μ„ λ¨λΈ"""
    
    # μ‚¬μ© κ°€λ¥ν• λ¨λΈλ“¤ (Microsoft λ¨λΈμ„ κΈ°λ³ΈμΌλ΅)
    AVAILABLE_MODELS = {
        "Nasserelsaman/microsoft-finetuned-personality": {
            "name": "Microsoft Finetuned Personality", 
            "description": "Microsoftμ—μ„ νμΈνλ‹ν• κ³ μ„±λ¥ μ„±κ²© λ¶„μ„ λ¨λΈ (μ¶”μ²)",
            "labels": ["μΉν™”μ„±", "μ„±μ‹¤μ„±", "μ™Έν–¥μ„±", "μ‹ κ²½μ„±", "κ°λ°©μ„±"]
        },
        "Minej/bert-base-personality": {
            "name": "Minej BERT Personality",
            "description": "BERT κΈ°λ° μ„±κ²© λ¶„μ„ λ¨λΈ",
            "labels": ["μΉν™”μ„±", "μ„±μ‹¤μ„±", "μ™Έν–¥μ„±", "μ‹ κ²½μ„±", "κ°λ°©μ„±"]
        }
    }
    
    def __init__(self, model_name: str = "Nasserelsaman/microsoft-finetuned-personality"):
        """
        BERT κ°μ • λ¶„μ„ λ¨λΈ μ΄κΈ°ν™”
        
        Args:
            model_name: HuggingFace λ¨λΈ μ΄λ¦„
        """
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"π”¥ λ””λ°”μ΄μ¤: {self.device}")
        print(f"π“¦ λ¨λΈ λ΅λ”© μ¤‘: {model_name}")
        
        # ν† ν¬λ‚μ΄μ €μ™€ λ¨λΈ λ΅λ“
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        # λ¨λΈλ³„ λ μ΄λΈ” μ„¤μ •
        if model_name in self.AVAILABLE_MODELS:
            self.emotion_labels = self.AVAILABLE_MODELS[model_name]["labels"]
        else:
            self.emotion_labels = ["κΈ°μ¨", "μ¬ν””", "λ¶„λ…Έ", "λ‘λ ¤μ›€", "λ†€λΌμ›€", "νμ¤", "μ¤‘λ¦½"]
        
        print("β… λ¨λΈ λ΅λ”© μ™„λ£!")
    
    @classmethod
    def get_available_models(cls):
        """μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ©λ΅ λ°ν™"""
        return cls.AVAILABLE_MODELS
    
    def predict_emotion(self, text: str) -> Dict[str, float]:
        """
        ν…μ¤νΈμ κ°μ •μ„ μμΈ΅ν•©λ‹λ‹¤.
        
        Args:
            text: λ¶„μ„ν•  ν…μ¤νΈ
            
        Returns:
            κ°μ •λ³„ ν™•λ¥  λ”•μ…”λ„λ¦¬
        """
        # ν† ν¬λ‚μ΄μ§•
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=512,
            truncation=True,
            padding=True
        )
        
        # GPUλ΅ μ΄λ™
        inputs = {key: value.to(self.device) for key, value in inputs.items()}
        
        # μμΈ΅
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # κ²°κ³Ό λ³€ν™
        probabilities = predictions.cpu().numpy()[0]
        
        # λ¨λΈλ³„ λΌλ²¨ λ§¤ν•‘ μ²λ¦¬
        num_classes = len(probabilities)
        
        # Big5 μ„±κ²© λ¨λΈμ κ²½μ° λ¨λΈλ³„ λΌλ²¨ μμ„μ— λ§κ² λ§¤ν•‘
        if num_classes == 5:
            # Microsoft λ¨λΈ: LABEL_0=Agreeableness, LABEL_1=Conscientiousness, LABEL_2=Extraversion, LABEL_3=Neuroticism, LABEL_4=Openness
            if "microsoft" in self.model_name.lower():
                labels = ['μΉν™”μ„±', 'μ„±μ‹¤μ„±', 'μ™Έν–¥μ„±', 'μ‹ κ²½μ„±', 'κ°λ°©μ„±']
            else:
                # λ‹¤λ¥Έ λ¨λΈλ“¤μ κΈ°λ³Έ μμ„ (Minej λ¨λΈ λ“±)
                labels = ['μΉν™”μ„±', 'μ„±μ‹¤μ„±', 'μ™Έν–¥μ„±', 'μ‹ κ²½μ„±', 'κ°λ°©μ„±']
        elif len(probabilities) == len(self.emotion_labels):
            labels = self.emotion_labels
        else:
            labels = [f"κ°μ •_{i}" for i in range(num_classes)]
        
        result = {label: float(prob) for label, prob in zip(labels, probabilities)}
        return result
    
    def predict_batch(self, texts: List[str]) -> List[Dict[str, float]]:
        """
        μ—¬λ¬ ν…μ¤νΈμ κ°μ •μ„ λ°°μΉλ΅ μμΈ΅ν•©λ‹λ‹¤.
        
        Args:
            texts: λ¶„μ„ν•  ν…μ¤νΈ λ¦¬μ¤νΈ
            
        Returns:
            κ° ν…μ¤νΈλ³„ κ°μ • ν™•λ¥  λ¦¬μ¤νΈ
        """
        results = []
        for text in texts:
            result = self.predict_emotion(text)
            results.append(result)
        return results
    
    def get_top_emotion(self, text: str) -> Tuple[str, float]:
        """
        ν…μ¤νΈμ κ°€μ¥ λ†’μ€ ν™•λ¥ μ κ°μ •μ„ λ°ν™ν•©λ‹λ‹¤.
        
        Args:
            text: λ¶„μ„ν•  ν…μ¤νΈ
            
        Returns:
            (κ°μ •λ…, ν™•λ¥ ) νν”
        """
        emotions = self.predict_emotion(text)
        top_emotion = max(emotions.items(), key=lambda x: x[1])
        return top_emotion
    
    def predict(self, text: str) -> Dict[str, float]:
        """
        predict_emotionμ λ³„μΉ­ λ©”μ„λ“ (νΈν™μ„±μ„ μ„ν•΄)
        
        Args:
            text: λ¶„μ„ν•  ν…μ¤νΈ
            
        Returns:
            κ°μ •λ³„ ν™•λ¥  λ”•μ…”λ„λ¦¬
        """
        return self.predict_emotion(text)

# μ‚¬μ© μμ‹
if __name__ == "__main__":
    # λ¨λΈ μ΄κΈ°ν™”
    detector = BERTEmotionDetector()
    
    # ν…μ¤νΈ ν…μ¤νΈ
    test_texts = [
        "μ¤λ μ •λ§ κΈ°λ¶„μ΄ μΆ‹μ•„μ”! μƒλ΅μ΄ ν”„λ΅μ νΈκ°€ μ„±κ³µμ μΌλ΅ μ™„λ£λμ—μ–΄μ”.",
        "λ„λ¬΄ μ¬ν”„κ³  μ°μΈν•΄μ”. λ¨λ“  κ²ƒμ΄ μλ»λμ–΄κ°€λ” κ²ƒ κ°™μ•„μ”.",
        "μ΄κ±΄ μ •λ§ λ§λ„ μ• λΌ! λ„λ¬΄ ν™”κ°€ λ‚μ”!",
        "λ‚΄μΌ λ°ν‘κ°€ μμ–΄μ„ λ„λ¬΄ κ±±μ •λκ³  λ¬΄μ„μ›μ”.",
        "μ™€! μ •λ§ λ†€λλ„¤μ”. μ΄λ° μΌμ΄ μΌμ–΄λ‚  μ¤„ λ°λμ–΄μ”."
    ]
    
    print("\nπ­ κ°μ • λ¶„μ„ κ²°κ³Ό:")
    print("=" * 50)
    
    for text in test_texts:
        print(f"\nπ“ ν…μ¤νΈ: {text}")
        
        # μ „μ²΄ κ°μ • λ¶„μ„
        emotions = detector.predict_emotion(text)
        print("π“ κ°μ • λ¶„ν¬:")
        for emotion, prob in emotions.items():
            print(f"  {emotion}: {prob:.3f}")
        
        # μµκ³  κ°μ •
        top_emotion, top_prob = detector.get_top_emotion(text)
        print(f"π† μ£Όμ” κ°μ •: {top_emotion} ({top_prob:.3f})")
        print("-" * 30)